##Import Data
library(caret)
## Warning: package 'caret' was built under R version 3.4.4
## Loading required package: lattice
## Loading required package: ggplot2
## Warning: package 'ggplot2' was built under R version 3.4.4
library(corrplot)
## corrplot 0.84 loaded
build_data <- read.csv('https://raw.githubusercontent.com/_____________')
results_data <- read.csv('https://raw.githubusercontent.com/______________')

class(build_data);class(results_data)
## [1] "data.frame"
## [1] "data.frame"
dim(build_data);dim(results_data)
## [1] 608  30
## [1]  5 29
names(build_data);names(results_data)
##  [1] "property_id" "AG"          "CP"          "DCK"         "DCK2"       
##  [6] "DCKC"        "ENC"         "GAR"         "LA"          "LA1"        
## [11] "LA2"         "OP"          "OP2"         "OPP"         "PA"         
## [16] "PAC"         "PTO"         "RMS"         "RSH"         "RSW"        
## [21] "SPA"         "UTL"         "UTL2"        "WD"          "WDD"        
## [26] "year_built"  "acres"       "eff_front"   "eff_depth"   "value"
##  [1] "property_id" "AG"          "CP"          "DCK"         "DCK2"       
##  [6] "DCKC"        "ENC"         "GAR"         "LA"          "LA1"        
## [11] "LA2"         "OP"          "OP2"         "OPP"         "PA"         
## [16] "PAC"         "PTO"         "RMS"         "RSH"         "RSW"        
## [21] "SPA"         "UTL"         "UTL2"        "WD"          "WDD"        
## [26] "year_built"  "acres"       "eff_front"   "eff_depth"
str(build_data);str(results_data)
## 'data.frame':    608 obs. of  30 variables:
##  $ property_id: int  1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 ...
##  $ AG         : num  240 462 455 438 438 ...
##  $ CP         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ DCK        : num  200 0 0 0 0 0 0 0 228 0 ...
##  $ DCK2       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ DCKC       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ ENC        : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ GAR        : int  0 0 0 0 0 0 0 484 0 0 ...
##  $ LA         : num  1241 1889 1712 1629 1211 ...
##  $ LA1        : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ LA2        : num  0 0 0 0 527 0 0 980 0 0 ...
##  $ OP         : num  64 89 280 50 264 ...
##  $ OP2        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ OPP        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ PA         : num  0 40 99 264 0 ...
##  $ PAC        : num  0 0 0 80 0 0 0 0 0 0 ...
##  $ PTO        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ RMS        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ RSH        : int  0 0 0 0 0 0 0 0 80 0 ...
##  $ RSW        : int  0 0 0 0 0 0 0 0 0 480 ...
##  $ SPA        : int  0 0 0 0 0 0 0 0 0 1 ...
##  $ UTL        : int  0 60 53 54 0 60 0 0 0 0 ...
##  $ UTL2       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ WD         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ WDD        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ year_built : int  2001 1984 1984 1984 1984 1984 1993 1984 1993 2012 ...
##  $ acres      : num  0.0869 0.2363 0.1543 0.1515 0.1653 ...
##  $ eff_front  : int  0 0 56 55 60 60 65 0 0 0 ...
##  $ eff_depth  : int  0 0 120 120 120 120 120 0 0 0 ...
##  $ value      : int  160860 210390 181040 192180 189430 169840 190070 211130 228670 307280 ...
## 'data.frame':    5 obs. of  29 variables:
##  $ property_id: int  2001 2002 2003 2004 2005
##  $ AG         : int  476 360 420 420 399
##  $ CP         : int  0 0 0 0 0
##  $ DCK        : int  0 0 0 324 180
##  $ DCK2       : int  0 0 0 0 0
##  $ DCKC       : int  0 0 0 0 0
##  $ ENC        : int  0 0 0 0 0
##  $ GAR        : int  0 0 0 0 0
##  $ LA         : int  1658 2011 1760 2450 1922
##  $ LA1        : int  0 0 0 0 0
##  $ LA2        : int  0 0 0 0 0
##  $ OP         : int  242 32 96 33 147
##  $ OP2        : int  0 0 0 0 0
##  $ OPP        : int  0 0 0 0 0
##  $ PA         : int  120 0 90 0 0
##  $ PAC        : int  0 210 0 0 0
##  $ PTO        : int  0 0 0 0 0
##  $ RMS        : int  0 0 0 0 0
##  $ RSH        : int  0 0 0 0 100
##  $ RSW        : int  0 0 0 0 0
##  $ SPA        : int  0 0 0 0 0
##  $ UTL        : int  0 0 0 0 0
##  $ UTL2       : int  0 0 0 0 0
##  $ WD         : int  0 0 0 0 0
##  $ WDD        : int  0 0 0 0 0
##  $ year_built : int  1983 1991 1986 1992 2002
##  $ acres      : num  0.237 0.113 0.261 0.323 0.151
##  $ eff_front  : int  0 49 0 0 0
##  $ eff_depth  : int  0 100 0 0 0
#How many unique numbers
apply(build_data, 2, function(x) length(unique(x)))
## property_id          AG          CP         DCK        DCK2        DCKC 
##         608         119           2          96           4          19 
##         ENC         GAR          LA         LA1         LA2          OP 
##          11           8         357          12         139         163 
##         OP2         OPP          PA         PAC         PTO         RMS 
##           3           4         126          47           4           2 
##         RSH         RSW         SPA         UTL        UTL2          WD 
##          15          17           2          22           2           4 
##         WDD  year_built       acres   eff_front   eff_depth       value 
##           6          24         243          42          49         587
#What are the unique numbers
unique(build_data[, 3])
## [1]   0 372
unique(build_data[, 5])
## [1]   0 174  80 144
unique(build_data[, 6])
##  [1]   0 216 386 280 170 120 264 198 108 300 152 273 230 168 340 311 200
## [18] 144 192
unique(build_data[, 7])
##  [1]   0.0 208.0 211.5 100.0 154.0 180.0 299.0  99.0 224.0 144.0 300.0
unique(build_data[, 8])
## [1]   0 484 400 441 528 420 440 529
unique(build_data[, 10])
##  [1]   0.0  96.0 227.0 484.0  48.0 156.0  21.0 376.0 391.0 396.0 283.0
## [12] 307.5
unique(build_data[, 13])
## [1]  0 24 85
unique(build_data[, 14])
## [1]   0  98  49 144
unique(build_data[, 17])
## [1]   0 540 240 360
unique(build_data[, 18])
## [1] 0 1
unique(build_data[, 21])
## [1] 0 1
unique(build_data[, 23])
## [1]   0 160
unique(build_data[, 24])
## [1]   0 192  64 507
unique(build_data[, 25])
## [1]   0 150  60 252 100 320
#There are two features that has "0" and "1" as it's value: RMS and SPA

corbuild <- cor(build_data)
corrplot::corrplot(corbuild, type = "full")
 
findCorrelation(corbuild, cutoff = 0.75, verbose = TRUE, names = TRUE)
## Compare row 29  and column  28 with corr  0.955 
##   Means:  0.12 vs 0.064 so flagging column 29 
## Compare row 2  and column  8 with corr  0.781 
##   Means:  0.086 vs 0.062 so flagging column 2 
## All correlations <= 0.75
## [1] "eff_depth" "AG"
#"eff_depth" "AG" have high correlations.
Data Preprocessing
nearZeroVar(build_data)
##  [1]  3  5  6  7  8 10 13 14 16 17 18 19 20 21 22 23 24 25
# 3  5  6  7  8 10 13 14 16 17 18 19 20 21 22 23 24 25

#Removing near zero variance and property_id
cleaning_buildData <- build_data[, c(-1, -3, -5, -6, -7, -8, -10, -13, -14, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25)]

#StepAIC
library(MASS)
# Fit the full model 
full.model <- glm(value ~., data = cleaning_buildData, family = "gaussian")
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)
summary(step.model)
## 
## Call:
## glm(formula = value ~ AG + DCK + LA + LA2 + OP + year_built + 
##     acres + eff_front + eff_depth, family = "gaussian", data = cleaning_buildData)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -43733   -6147     354    5961   44016  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.886e+06  1.431e+05 -13.178  < 2e-16 ***
## AG           1.061e+01  4.178e+00   2.541   0.0113 *  
## DCK          5.115e+00  3.184e+00   1.606   0.1087    
## LA           6.172e+01  1.498e+00  41.193  < 2e-16 ***
## LA2          4.957e+01  1.080e+00  45.893  < 2e-16 ***
## OP           2.146e+01  5.361e+00   4.003 7.05e-05 ***
## year_built   9.825e+02  7.191e+01  13.664  < 2e-16 ***
## acres        9.450e+04  1.085e+04   8.707  < 2e-16 ***
## eff_front    4.763e+02  5.307e+01   8.975  < 2e-16 ***
## eff_depth   -2.371e+02  2.752e+01  -8.615  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 106655754)
## 
##     Null deviance: 6.8492e+11  on 607  degrees of freedom
## Residual deviance: 6.3780e+10  on 598  degrees of freedom
## AIC: 12976
## 
## Number of Fisher Scoring iterations: 2
step.model$anova
## Stepwise Model Path 
## Analysis of Deviance Table
## 
## Initial Model:
## value ~ AG + DCK + LA + LA2 + OP + PA + year_built + acres + 
##     eff_front + eff_depth
## 
## Final Model:
## value ~ AG + DCK + LA + LA2 + OP + year_built + acres + eff_front + 
##     eff_depth
## 
## 
##   Step Df Deviance Resid. Df  Resid. Dev      AIC
## 1                        597 63737875385 12977.89
## 2 - PA  1 42265637       598 63780141022 12976.30
#Final Model:
#value ~ AG + DCK + LA + LA2 + OP + year_built + acres + eff_front + 
#    eff_depth


#  Step Df Deviance Resid. Df  Resid. Dev      AIC
#1                        597 63737875385 12977.89
#2 - PA  1 42265637       598 63780141022 12976.30


#Adding column as TotalSQFT
cleaning_buildData$TotalSQFT <- rowSums(build_data[, 2:25])

#Adding column as features
cleaning_buildData$features <- rowSums(build_data[, 2:25] != 0)

#Create new column called "Age" for the age of the house

cleaning_buildData$currentYear <- format(Sys.Date(), "%Y") 
cleaning_buildData$currentYear <- as.numeric(cleaning_buildData$currentYear)

cleaning_buildData$age <- cleaning_buildData$currentYear - cleaning_buildData$year_built


#Removing columns in our cleaned data
cleaned_buildData <- cleaning_buildData[, -14]

#Seeing the correlation

corclean <- cor(cleaned_buildData)
corrplot::corrplot(corclean, type = "full")
 
findCorrelation(corclean, cutoff = 0.75, verbose = TRUE, names = TRUE)
## Compare row 12  and column  11 with corr  0.908 
##   Means:  0.355 vs 0.193 so flagging column 12 
## Compare row 10  and column  9 with corr  0.955 
##   Means:  0.235 vs 0.178 so flagging column 10 
## Compare row 7  and column  14 with corr  1 
##   Means:  0.197 vs 0.167 so flagging column 7 
## All correlations <= 0.75
## [1] "TotalSQFT"  "eff_depth"  "year_built"
#TotalSQFT has high correalation as well as eff_depth and year_built
#We can use eff_front or eff_depth

final_buildData <- cleaned_buildData[, c(-7, -10)] #dropped year_built and eff_depth
corrplot(cor(final_buildData))
 
hist(cleaned_buildData$value) #It looks pretty normally distributed
 
plot(final_buildData)
 
summary(final_buildData)
##        AG             DCK               LA            LA2        
##  Min.   :  0.0   Min.   :  0.00   Min.   : 650   Min.   :   0.0  
##  1st Qu.:397.0   1st Qu.:  0.00   1st Qu.:1346   1st Qu.:   0.0  
##  Median :418.0   Median :  0.00   Median :1596   Median :   0.0  
##  Mean   :401.6   Mean   : 60.06   Mean   :1627   Mean   : 420.5  
##  3rd Qu.:444.0   3rd Qu.:  0.00   3rd Qu.:1889   3rd Qu.: 958.2  
##  Max.   :718.0   Max.   :892.00   Max.   :2809   Max.   :2000.0  
##        OP               PA              acres          eff_front     
##  Min.   :  0.00   Min.   :   0.00   Min.   :0.0744   Min.   :  0.00  
##  1st Qu.: 25.00   1st Qu.:   0.00   1st Qu.:0.1171   1st Qu.:  0.00  
##  Median : 40.00   Median :   0.00   Median :0.1534   Median : 45.00  
##  Mean   : 70.91   Mean   :  79.92   Mean   :0.1609   Mean   : 37.61  
##  3rd Qu.: 81.25   3rd Qu.: 132.00   3rd Qu.:0.1832   3rd Qu.: 60.00  
##  Max.   :686.50   Max.   :1050.00   Max.   :0.4038   Max.   :109.00  
##      value          TotalSQFT       features           age       
##  Min.   :122950   Min.   :1496   Min.   : 3.000   Min.   : 6.00  
##  1st Qu.:190970   1st Qu.:2354   1st Qu.: 4.000   1st Qu.:21.00  
##  Median :215260   Median :2680   Median : 4.000   Median :27.00  
##  Mean   :213456   Mean   :2756   Mean   : 4.613   Mean   :26.61  
##  3rd Qu.:233312   3rd Qu.:3073   3rd Qu.: 5.000   3rd Qu.:32.00  
##  Max.   :340530   Max.   :5302   Max.   :10.000   Max.   :35.00
#Should I scale and center?
#final_buildData$scaled <- scale(final_buildData$value, center = TRUE, scale = TRUE)
#summary(final_buildData); Mean = 0
#sd(final_buildData$scaled); SD = 1
#fit <- lm(value~., final_buildData[, -7])
#plot(fit)
#fit2 <- lm(scaled~., final_buildData[, -3])
#plot(fit2)
#After comparing Normal Q-Q plot in the centered and scaled values, there were no differences from the original values. Therefor, center and scaling would not be beneficial

#2nd stepAIC-------------------------

nearZeroVar(final_buildData) #-- no nearZeroVar
## integer(0)
library(MASS)
# Fit the full model 
full.model2 <- glm(value ~., data = final_buildData, family = "gaussian")
# Stepwise regression model
step.model2 <- stepAIC(full.model2, direction = "both", 
                      trace = FALSE)
summary(step.model2)
## 
## Call:
## glm(formula = value ~ DCK + LA + LA2 + OP + PA + acres + eff_front + 
##     TotalSQFT + features + age, family = "gaussian", data = final_buildData)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -47283   -6607     824    7096   27360  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 83572.144   3250.462  25.711  < 2e-16 ***
## DCK           -24.020      3.747  -6.410 2.95e-10 ***
## LA             28.219      3.608   7.821 2.38e-14 ***
## LA2            15.391      3.171   4.854 1.55e-06 ***
## OP             -8.576      5.953  -1.441 0.150210    
## PA            -29.405      3.846  -7.645 8.39e-14 ***
## acres       81061.104  10041.202   8.073 3.80e-15 ***
## eff_front      57.052     15.621   3.652 0.000283 ***
## TotalSQFT      33.142      3.102  10.684  < 2e-16 ***
## features     1003.816    605.470   1.658 0.097860 .  
## age         -1100.501     67.523 -16.298  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 88936505)
## 
##     Null deviance: 6.8492e+11  on 607  degrees of freedom
## Residual deviance: 5.3095e+10  on 597  degrees of freedom
## AIC: 12867
## 
## Number of Fisher Scoring iterations: 2
step.model2$anova
## Stepwise Model Path 
## Analysis of Deviance Table
## 
## Initial Model:
## value ~ AG + DCK + LA + LA2 + OP + PA + acres + eff_front + TotalSQFT + 
##     features + age
## 
## Final Model:
## value ~ DCK + LA + LA2 + OP + PA + acres + eff_front + TotalSQFT + 
##     features + age
## 
## 
##   Step Df Deviance Resid. Df  Resid. Dev      AIC
## 1                        596 53080030622 12868.64
## 2 - AG  1 15063030       597 53095093653 12866.82
#Final Model:
#value ~ DCK + LA + LA2 + OP + PA + acres + eff_front + TotalSQFT + 
#    features + age

#10 variables included in final model. It's just 2 variables off from the final data, so I decided to use all the variables on my cleaned data
Sample Selection - Generate a Training and Test Set
set.seed(123)
trainingIndex <- createDataPartition(final_buildData$value, p = .80, list= FALSE)
trainSet = final_buildData[trainingIndex,]
testSet = final_buildData[-trainingIndex,]
Model Building / Training / Tuning
library(e1071)

control <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

metric <- "RMSE"

#Using Linear Model----------------------------------------------------------
lmfit <- lm(value~., data=trainSet)
predlm <- predict(lmfit, data=testSet)


set.seed(123)
lmFit1 <- train(value ~ ., data = trainSet,
                 method = "lm",
                 trControl = control,
                 metric = metric,
                 verbose = FALSE)
lmFit1
## Linear Regression 
## 
## 488 samples
##  11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 440, 440, 438, 438, 440, 439, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   9335.184  0.9237635  7498.004
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
#RMSE      Rsquared   MAE     
#  9335.184  0.9237635  7498.004

#Using Super Vector Machine----------------------------------------------------
set.seed(123)
svmFit1 <- train(value ~ ., data = trainSet,
                 method = "svmRadial",
                 trControl = control,
                 #metric = metric,
                 verbose = FALSE)
svmFit1
## Support Vector Machines with Radial Basis Function Kernel 
## 
## 488 samples
##  11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 440, 440, 438, 438, 440, 439, ... 
## Resampling results across tuning parameters:
## 
##   C     RMSE      Rsquared   MAE     
##   0.25  12786.73  0.8628938  8263.103
##   0.50  11832.94  0.8799345  7657.240
##   1.00  11111.43  0.8937211  7212.348
## 
## Tuning parameter 'sigma' was held constant at a value of 0.07996151
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.07996151 and C = 1.
#C: 1.00, RMSE: 11111.43, R2: 0.8937211, MAE: 7212.348, sigma: 0.07996151

#SVM Polynomial --------------------------------
set.seed(123)
svmPFit1 <- train(value ~ ., data = trainSet,
                 method = "svmPoly",
                 trControl = control,
                 #metric = metric,
                 verbose = FALSE)
svmPFit1
## Support Vector Machines with Polynomial Kernel 
## 
## 488 samples
##  11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 440, 440, 438, 438, 440, 439, ... 
## Resampling results across tuning parameters:
## 
##   degree  scale  C     RMSE       Rsquared   MAE      
##   1       0.001  0.25  28318.920  0.8320030  22356.240
##   1       0.001  0.50  24076.498  0.8483848  19028.558
##   1       0.001  1.00  17965.419  0.8747240  14151.169
##   1       0.010  0.25  12092.382  0.9101594   9500.029
##   1       0.010  0.50  10263.722  0.9192107   8087.648
##   1       0.010  1.00   9628.054  0.9223454   7578.290
##   1       0.100  0.25   9497.868  0.9228325   7381.057
##   1       0.100  0.50   9471.480  0.9230962   7321.999
##   1       0.100  1.00   9479.334  0.9231155   7313.810
##   2       0.001  0.25  24069.607  0.8481876  19023.281
##   2       0.001  0.50  17957.908  0.8746187  14144.989
##   2       0.001  1.00  13089.168  0.9046165  10249.147
##   2       0.010  0.25   9784.093  0.9247143   7723.781
##   2       0.010  0.50   8979.683  0.9299886   7000.898
##   2       0.010  1.00   8765.659  0.9319905   6663.562
##   2       0.100  0.25   8960.613  0.9272904   6330.940
##   2       0.100  0.50   9050.648  0.9251767   6331.543
##   2       0.100  1.00   9134.526  0.9234328   6367.733
##   3       0.001  0.25  20502.833  0.8612929  16187.435
##   3       0.001  0.50  14818.701  0.8941376  11636.919
##   3       0.001  1.00  11447.661  0.9143591   9020.722
##   3       0.010  0.25   9038.684  0.9298480   7026.963
##   3       0.010  0.50   8735.933  0.9328287   6593.698
##   3       0.010  1.00   8648.618  0.9339194   6427.954
##   3       0.100  0.25   9719.376  0.9159802   6555.085
##   3       0.100  0.50  10666.768  0.9003613   6951.034
##   3       0.100  1.00  12077.576  0.8758816   7552.331
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were degree = 3, scale = 0.01 and C
##  = 1.
#degree: 3, scale: 0.010, C: 1.00, RMSE: 8648.618, R2: 0.9339194, MAE: 6427.954

#Random Forest
set.seed(123)
rfFit1 <- train(value ~ ., data = trainSet,
                 method = "rf",
                 trControl = control,
                 metric = metric,
                 verbose = FALSE)
rfFit1
## Random Forest 
## 
## 488 samples
##  11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 440, 440, 438, 438, 440, 439, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    9798.533  0.9256564  6884.569
##    6    8706.594  0.9346667  6201.896
##   11    8927.263  0.9301427  6398.324
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.
#mtry:6, RMSE: 8706.594, R2: 0.9346667, MAE: 6201.896

#Using Generalized Boosted Regression Modeling--------------------------------
set.seed(123)
gbmFit1 <- train(value ~ ., data = trainSet,
                 method = "gbm",
                 trControl = control,
                 metric = metric,
                 verbose = FALSE)
gbmFit1
## Stochastic Gradient Boosting 
## 
## 488 samples
##  11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 440, 440, 438, 438, 440, 439, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE       Rsquared   MAE     
##   1                   50      11930.039  0.8889647  9080.934
##   1                  100      10004.811  0.9144538  7403.457
##   1                  150       9465.760  0.9221675  6927.769
##   2                   50      10085.188  0.9143101  7373.554
##   2                  100       9125.392  0.9277262  6589.055
##   2                  150       8921.683  0.9307894  6436.153
##   3                   50       9425.225  0.9236270  6780.110
##   3                  100       8860.482  0.9317910  6354.998
##   3                  150       8729.098  0.9337814  6252.788
## 
## Tuning parameter 'shrinkage' was held constant at a value of 0.1
## 
## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.
# interaction.depth: 3, n.tress: 150, RMSE: 8729.098, R2: 0.9337814, MAE: 6252.788

#Using Enet---------------------------------------------
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1),
                        .fraction = seq(.05, 1, length = 20))
set.seed(123)
enetTune <- train(value~., trainSet,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = control)
enetTune
## Elasticnet 
## 
## 488 samples
##  11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 440, 440, 438, 438, 440, 439, ... 
## Resampling results across tuning parameters:
## 
##   lambda  fraction  RMSE       Rsquared   MAE      
##   0.00    0.05      30582.620  0.8335965  24176.436
##   0.00    0.10      27882.421  0.8335965  22076.415
##   0.00    0.15      25261.501  0.8335965  20029.931
##   0.00    0.20      22747.380  0.8335965  18061.234
##   0.00    0.25      20379.951  0.8335965  16213.505
##   0.00    0.30      18217.369  0.8335965  14537.690
##   0.00    0.35      16343.242  0.8335992  13116.451
##   0.00    0.40      14869.600  0.8441455  11983.076
##   0.00    0.45      13591.550  0.8648535  10957.723
##   0.00    0.50      12507.664  0.8799225  10105.671
##   0.00    0.55      11625.503  0.8904663   9382.261
##   0.00    0.60      11043.859  0.8975633   8891.608
##   0.00    0.65      10522.857  0.9060248   8500.221
##   0.00    0.70      10111.640  0.9121540   8196.532
##   0.00    0.75       9844.060  0.9161050   7963.168
##   0.00    0.80       9626.684  0.9194379   7770.480
##   0.00    0.85       9474.509  0.9216720   7638.416
##   0.00    0.90       9388.149  0.9229206   7556.283
##   0.00    0.95       9334.955  0.9237248   7498.962
##   0.00    1.00       9335.184  0.9237635   7498.004
##   0.01    0.05      30581.486  0.8335965  24175.569
##   0.01    0.10      27880.226  0.8335965  22074.705
##   0.01    0.15      25258.349  0.8335965  20027.433
##   0.01    0.20      22743.419  0.8335965  18058.091
##   0.01    0.25      20375.402  0.8335965  16209.395
##   0.01    0.30      18212.550  0.8335965  14533.547
##   0.01    0.35      16338.480  0.8335973  13111.997
##   0.01    0.40      14866.119  0.8446508  11978.746
##   0.01    0.45      13589.227  0.8653441  10955.015
##   0.01    0.50      12503.504  0.8805704  10104.914
##   0.01    0.55      11621.161  0.8908944   9376.517
##   0.01    0.60      11038.639  0.8975043   8883.610
##   0.01    0.65      10522.529  0.9058682   8494.956
##   0.01    0.70      10129.770  0.9118549   8200.524
##   0.01    0.75       9838.857  0.9163094   7936.358
##   0.01    0.80       9622.983  0.9195120   7742.827
##   0.01    0.85       9467.599  0.9217727   7607.664
##   0.01    0.90       9368.601  0.9232206   7524.769
##   0.01    0.95       9315.145  0.9240261   7472.907
##   0.01    1.00       9327.287  0.9238945   7484.149
##   0.10    0.05      30539.839  0.8335965  24142.973
##   0.10    0.10      27799.115  0.8335965  22011.179
##   0.10    0.15      25141.047  0.8335965  19934.781
##   0.10    0.20      22594.921  0.8335965  17940.687
##   0.10    0.25      20203.439  0.8335965  16070.873
##   0.10    0.30      18029.122  0.8335965  14392.096
##   0.10    0.35      16164.273  0.8336443  12977.374
##   0.10    0.40      14747.900  0.8511027  11875.239
##   0.10    0.45      13486.160  0.8715602  10887.170
##   0.10    0.50      12400.312  0.8852332  10032.936
##   0.10    0.55      11545.497  0.8934620   9326.054
##   0.10    0.60      11031.250  0.8989205   8883.206
##   0.10    0.65      10636.569  0.9036546   8531.567
##   0.10    0.70      10324.977  0.9076514   8267.272
##   0.10    0.75      10018.888  0.9123206   8002.250
##   0.10    0.80       9785.649  0.9160357   7794.399
##   0.10    0.85       9630.308  0.9186439   7646.801
##   0.10    0.90       9524.635  0.9206157   7545.839
##   0.10    0.95       9482.994  0.9217321   7494.976
##   0.10    1.00       9498.499  0.9221521   7490.638
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were fraction = 0.95 and lambda = 0.01.
#lambda: 0.01, fraction: 0.95, RMSE: 9315.145, R2: 0.9240261, MAE: 7472.907
Model Selection
results <- resamples(list(LMfit=lmFit1, SVMradial=svmFit1, SVMpoly=svmPFit1, RF=rfFit1, GBM=gbmFit1, enet=enetTune))
summary(results)
## 
## Call:
## summary.resamples(object = results)
## 
## Models: LMfit, SVMradial, SVMpoly, RF, GBM, enet 
## Number of resamples: 100 
## 
## MAE 
##               Min.  1st Qu.   Median     Mean  3rd Qu.      Max. NA's
## LMfit     6027.863 6987.552 7354.583 7498.004 8058.746 10419.049    0
## SVMradial 4665.508 6280.629 7136.558 7212.348 7820.290 11050.759    0
## SVMpoly   4840.300 5749.170 6306.834 6427.954 7057.260  9056.071    0
## RF        4376.545 5681.456 6063.793 6201.896 6793.242  8432.570    0
## GBM       4125.407 5578.571 6158.275 6252.788 6880.499  8080.422    0
## enet      6006.582 6970.517 7337.282 7472.907 8046.515 10390.539    0
## 
## RMSE 
##               Min.  1st Qu.    Median      Mean   3rd Qu.     Max. NA's
## LMfit     7336.572 8388.819  9139.442  9335.184 10139.407 12856.45    0
## SVMradial 5843.662 8594.600 10086.835 11111.433 13209.327 23123.48    0
## SVMpoly   6189.636 7505.425  8446.785  8648.618  9711.795 12059.60    0
## RF        5774.513 7645.084  8661.605  8706.594  9717.326 13977.67    0
## GBM       5455.698 7352.812  8322.751  8729.098 10077.127 14725.97    0
## enet      7307.916 8345.640  9108.279  9315.145 10035.850 12772.12    0
## 
## Rsquared 
##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## LMfit     0.8535775 0.9081700 0.9289256 0.9237635 0.9401732 0.9581699    0
## SVMradial 0.7089412 0.8656461 0.9090215 0.8937211 0.9279067 0.9647453    0
## SVMpoly   0.8696433 0.9204357 0.9389075 0.9339194 0.9494138 0.9711908    0
## RF        0.8602093 0.9203282 0.9368183 0.9346667 0.9500392 0.9732877    0
## GBM       0.8447288 0.9210934 0.9357606 0.9337814 0.9505900 0.9726534    0
## enet      0.8543987 0.9105895 0.9287302 0.9240261 0.9404162 0.9586921    0
dotplot(results)
 
#Predicting on the validation set

#Linear Model
predictions.lm <- predict(lmFit1, testSet)
#Plot
plot(predictions.lm, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(predictions.lm, testSet$value)
## [1] 10172.18
#10172.182
R2(predictions.lm, testSet$value)
## [1] 0.9099982
#0.9099982

#SVM Radial
predictions.svm <- predict(svmFit1, testSet)
#Plot
plot(predictions.svm, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(predictions.svm, testSet$value)
## [1] 10794.11
#10794.108
R2(predictions.svm, testSet$value)
## [1] 0.9083285
#0.9083285  

#SVM Polynomial
predictions.svmP <- predict(svmPFit1, testSet)
#Plot
plot(predictions.svmP, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(predictions.svmP, testSet$value)
## [1] 9391.064
#9391.064
R2(predictions.svmP, testSet$value)
## [1] 0.9238125
#0.9238125

#Random Forest
predictions.rf <- predict(rfFit1, testSet)
#Plot
plot(predictions.rf, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(predictions.rf, testSet$value)
## [1] 9066.97
#9066.970
R2(predictions.rf, testSet$value)
## [1] 0.9293788
#0.9293788

#Enet
enetpred <- predict(enetTune, testSet)
#Plot
plot(enetpred, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(enetpred, testSet$value)
## [1] 10270.88
#10270.877
R2(enetpred, testSet$value)
## [1] 0.9081977
#0.9081977  

#GBM 
predictions.gbm <- predict(gbmFit1, testSet)
#Plot
plot(predictions.gbm, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(predictions.gbm, testSet$value)
## [1] 8174.917
#8174.917
R2(predictions.gbm, testSet$value)
## [1] 0.9419783
#0.9419783  

rmse <- matrix(c(RMSE(predictions.lm, testSet$value), RMSE(predictions.svm, testSet$value), RMSE(predictions.svmP, testSet$value), RMSE(predictions.rf, testSet$value),RMSE(enetpred, testSet$value), RMSE(predictions.gbm, testSet$value)))
R2 <- matrix(c(R2(predictions.lm, testSet$value), R2(predictions.svm, testSet$value), R2(predictions.svmP, testSet$value), R2(predictions.rf, testSet$value), R2(enetpred, testSet$value), R2(predictions.gbm, testSet$value)))
resultspred <- cbind.data.frame(rmse,R2);resultspred
##        rmse        R2
## 1 10172.182 0.9099982
## 2 10794.108 0.9083285
## 3  9391.064 0.9238125
## 4  9066.970 0.9293788
## 5 10270.877 0.9081977
## 6  8174.917 0.9419783
#GBM has the lowest RMSE and higherst R2
What is the expected accuracy of your approach (R^2 / RMSE / MAE etc.)?
#GBM 
predictions.gbm <- predict(gbmFit1, testSet)
#Plot
plot(predictions.gbm, testSet$value,
     xlab="predicted", ylab="actual")
abline(a=0, b=1)
 
#RMSE prediction
RMSE(predictions.gbm, testSet$value)
## [1] 8174.917
#8174.917
R2(predictions.gbm, testSet$value)
## [1] 0.9419783
#0.9419783
MAE(predictions.gbm, testSet$value)
## [1] 5502.878
#5502.878

#Expected Accuracy of my approach is R2 = 0.9419783, RMSE = 8174.917, and MAE = 5502.878
Predict the House Values for results_data
#Fixing results_data to be the same lenght as build_data
#Removing columns
cleaning_results_data <- results_data[, c(-1, -3, -5, -6, -7, -8, -10, -13, -14, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25)]

#Adding column TotalSQFT
cleaning_results_data$TotalSQFT <- rowSums(results_data[, 2:25])

#Adding column features
cleaning_results_data$features <- rowSums(results_data[, 2:25] != 0)

#Create new column called "Age" for the age of the house

cleaning_results_data$currentYear <- format(Sys.Date(), "%Y") 
cleaning_results_data$currentYear <- as.numeric(cleaning_results_data$currentYear)

cleaning_results_data$age <- cleaning_results_data$currentYear - cleaning_results_data$year_built


#Removing columns in our cleaned data
cleaned_results_data <-  cleaning_results_data[, -13]
final_results_data <- cleaned_results_data[, c(-7, -10)]

#Prediction on final_data
predictions.gbmR <- predict(gbmFit1, final_results_data)
predictions.gbmR
## [1] 196245.2 220130.8 201494.8 240544.1 224201.9
#196245.2 220130.8 201494.8 240544.1 224201.9
value <- c(196245.2, 220130.8, 201494.8, 240544.1, 224201.9)
property_id <- results_data$property_id

cbind(property_id, value)
##      Property predvalue
## [1,]     2001  196245.2
## [2,]     2002  220130.8
## [3,]     2003  201494.8
## [4,]     2004  240544.1
## [5,]     2005  224201.9
#install.packages("rtf")
library(rtf)
rtffile <- RTF("/Users/hannina/Documents/____________________")
addParagraph(rtffile, "This is the Prediction of House Values:\n")
addTable(rtffile, cbind(property_id, value))
done(rtffile)
