a.	For the spam data, partition the data into 2/3 training and 1/3 test data.
library(ElemStatLearn);data(spam)
library(e1071) 
library(ISLR)
set.seed(123)
train <- sample(1:nrow(spam), nrow(spam)*2/3)
spam.train <- spam[train,]
spam.test <- spam[-train,]
b.	Build the best svm model for spam training data using all the variables for response variable spam. Select among choices of kernel functions (polynomial or radial) and cost (.1, 1, 10, 20, 30, 40, 100). For radial kernel choose gamma (0.01, 0.05, 0.1, 1, 5) and for polynomial kernel choose degree of polynomial (1 or 2 or 3).
#Radial - best parameters: cost = 10, gamma = 0.01 and error = 0.0655415
set.seed(123)
tune.out=tune(svm, spam~. , data=spam.train, kernel="radial", 
              ranges=list(cost=c(0.1, 1, 10, 20, 30, 40, 100), 
                          gamma=c(0.01,0.05,0.1,5)))
summary(tune.out)
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##    10  0.01
## 
## - best performance: 0.0655415 
## 
## - Detailed performance results:
##     cost gamma      error dispersion
## 1    0.1  0.01 0.10532776 0.01531755
## 2    1.0  0.01 0.07630346 0.01072591
## 3   10.0  0.01 0.06554150 0.01456084
## 4   20.0  0.01 0.06586085 0.01354354
## 5   30.0  0.01 0.06586191 0.01245799
## 6   40.0  0.01 0.06716591 0.01286417
## 7  100.0  0.01 0.06749058 0.01312322
## 8    0.1  0.05 0.12619595 0.01977723
## 9    1.0  0.05 0.08184518 0.01498189
## 10  10.0  0.05 0.08118946 0.01272768
## 11  20.0  0.05 0.08413063 0.01708596
## 12  30.0  0.05 0.08380064 0.01525161
## 13  40.0  0.05 0.08314705 0.01666964
## 14 100.0  0.05 0.08835877 0.01994018
## 15   0.1  0.10 0.20184476 0.02521725
## 16   1.0  0.10 0.09912819 0.01331896
## 17  10.0  0.10 0.09586660 0.01537345
## 18  20.0  0.10 0.10043218 0.02151393
## 19  30.0  0.10 0.10205978 0.01949139
## 20  40.0  0.10 0.10205765 0.01783379
## 21 100.0  0.10 0.10205339 0.01627289
## 22   0.1  5.00 0.38734964 0.03223956
## 23   1.0  5.00 0.26150497 0.03008989
## 24  10.0  5.00 0.25628899 0.02935435
## 25  20.0  5.00 0.25661578 0.02931494
## 26  30.0  5.00 0.25661578 0.02931494
## 27  40.0  5.00 0.25661578 0.02931494
## 28 100.0  5.00 0.25661578 0.02931494
#Polynomial - best parameters: ccost 10, degree = 1 and error = 0.07369
set.seed(123)
tune.out2=tune(svm, spam~. ,data=spam.train,kernel="polynomial",
              ranges=list(cost=c(0.1, 1, 10, 20, 30, 40, 100),
                          degree=c(1,2,3)))
summary(tune.out2)
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost degree
##    10      1
## 
## - best performance: 0.07369228 
## 
## - Detailed performance results:
##     cost degree      error  dispersion
## 1    0.1      1 0.10825722 0.012060631
## 2    1.0      1 0.08087437 0.014411705
## 3   10.0      1 0.07369228 0.006610919
## 4   20.0      1 0.07401908 0.005836686
## 5   30.0      1 0.07499521 0.007081383
## 6   40.0      1 0.07434268 0.007040428
## 7  100.0      1 0.07434162 0.007195098
## 8    0.1      2 0.27290243 0.027112937
## 9    1.0      2 0.16563518 0.019912371
## 10  10.0      2 0.09879500 0.012205793
## 11  20.0      2 0.09032275 0.014621081
## 12  30.0      2 0.08705904 0.012693469
## 13  40.0      2 0.08706117 0.011745961
## 14 100.0      2 0.09358115 0.009892056
## 15   0.1      3 0.30649337 0.033564953
## 16   1.0      3 0.23312256 0.028830617
## 17  10.0      3 0.14900364 0.012642659
## 18  20.0      3 0.12976943 0.012182217
## 19  30.0      3 0.11346576 0.011370029
## 20  40.0      3 0.10369270 0.015103354
## 21 100.0      3 0.09194929 0.013404000
#Radial has a smaller error rate than polynomial. 
c.	Apply the best model in b to the test data and get the confusion matrix and error rate.
#Best model: radial
bestmod=tune.out$best.model
summary(bestmod)
## 
## Call:
## best.tune(method = svm, train.x = spam ~ ., data = spam.train, 
##     ranges = list(cost = c(0.1, 1, 10, 20, 30, 40, 100), gamma = c(0.01, 
##         0.05, 0.1, 5)), kernel = "radial")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  10 
##       gamma:  0.01 
## 
## Number of Support Vectors:  695
## 
##  ( 334 361 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  email spam
#Confusion Matrix
ypred=predict(bestmod,spam.test)
table(predict=ypred, truth=spam.test$spam)
##        truth
## predict email spam
##   email   886   70
##   spam     30  548
#Error rate = 0.06518, which is slightly better than the spam training error rate (0.06554)
mean(ypred!=spam.test$spam)
## [1] 0.06518905
d.	Compare the results with randomForest.
# Random model for spam training data using all the variables (From HW 5)
library(randomForest)
## randomForest 4.6-12
## Type rfNews() to see new features/changes/bug fixes.
library(boot)
m = round(sqrt(dim(spam.train)[2]-1))
for (i in c(500,1000,2000)){ 
  for (j in c(m-1, m, m+1)){ 
    set.seed(123)
    rf.spam=randomForest(spam ~., data=spam.train,  mtry=j, ntree=i)
    # oob error rate for training data
    yhat=rf.spam$predicted
    y=spam.train$spam
    error_rate <- mean(y != yhat)
    if (exists('oob_err')==FALSE){
      oob_err = c(i,j,error_rate) 
    }
    else{
      oob_err = rbind(oob_err, c(i,j,error_rate)) 
    }
  }
}
oob_err <- as.data.frame(oob_err) 
names(oob_err) <- c('ntree', 'mtry', 'oob_error_rate') 
oob_err$mtry <- as.factor(oob_err$mtry) 
min.ntree <- oob_err$ntree[which.min(oob_err$oob_error_rate)] 
min.ntree
## [1] 500
min.mtry <- as.numeric(levels(oob_err$mtry[which.min(oob_err$oob_error_rate)])[oob_err$mtry[which.min(oob_err$oob_error_rate)]]) 
min.mtry
## [1] 8
min(oob_err$oob_error_rate)
## [1] 0.04597326
# Best mtry is 8 and best ntree is 500. OOB error rate for the training data is 0.045973

set.seed(123)
rf.spam=randomForest(spam ~., data=spam.train,  importance=TRUE, mtry=min.mtry, ntree=min.ntree)
#Confusion Matrix
yhat <- predict(rf.spam, spam.test, type = 'class')
y <- spam.test$spam
table(y,yhat) 
##        yhat
## y       email spam
##   email   893   23
##   spam     57  561
#Error Rate = 0.052151
mean(y != yhat)
## [1] 0.05215124
#Based on the result, rainforest error rate (0.0521) for test data is better than radial error rate (0.6518).
e.	Suppose data=data.frame(x=c(1,2,4,5,9,10,14,19),y=c(-1,-1,-1,-1,1,1,1,1))).
x = c(1,2,4,5,9,10,14,19)
y = c(-1,-1,-1,-1,1,1,1,1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue", "blue")
plot(x, y, col = colors, xlim = c(0, 20), ylim = c(-2, 2))
